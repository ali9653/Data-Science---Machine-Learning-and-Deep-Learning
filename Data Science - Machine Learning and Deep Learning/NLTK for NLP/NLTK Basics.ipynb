{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM POS TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged Sentence:\n",
      "  word= I \ttag= PPSS\n",
      "  word= WANT \ttag= VB\n",
      "  word= TO \ttag= TO\n",
      "  word= RACE \ttag= VB\n"
     ]
    }
   ],
   "source": [
    "# OK, here is our HMM POS Tagger for this example\n",
    "\n",
    "# states\n",
    "start = -1; VB = 0; TO = 1; NN = 2; PPSS = 3; stateCount = 4\n",
    "stateNames = [\"VB\", \"TO\", \"NN\", \"PPSS\"]\n",
    "\n",
    "# outputs\n",
    "I = 0; WANT = 1; To = 2; RACE=3\n",
    "\n",
    "timeSteps = 4\n",
    "\n",
    "# state transition probabilities\n",
    "trans = {}\n",
    "trans[(start, VB)] = .19\n",
    "trans[(start, TO)] = .0043\n",
    "trans[(start, NN)] = .041\n",
    "trans[(start, PPSS)] = .067\n",
    "\n",
    "trans[(VB, VB)] = .0038\n",
    "trans[(VB, TO)] = .035\n",
    "trans[(VB, NN)] = .047\n",
    "trans[(VB, PPSS)] = .0070\n",
    "\n",
    "trans[(TO, VB)] = .83\n",
    "trans[(TO, TO)] = 0\n",
    "trans[(TO, NN)] = .00047\n",
    "trans[(TO, PPSS)] = 0\n",
    "\n",
    "trans[(NN, VB)] = .0040\n",
    "trans[(NN, TO)] = .016\n",
    "trans[(NN, NN)] = .087\n",
    "trans[(NN, PPSS)] = .0045\n",
    "\n",
    "trans[(PPSS, VB)] = .23\n",
    "trans[(PPSS, TO)] = .00079\n",
    "trans[(PPSS, NN)] = .0012\n",
    "trans[(PPSS, PPSS)] = .00014\n",
    "\n",
    "# state outputs\n",
    "output = {}\n",
    "output[(VB, I)] = 0\n",
    "output[(VB, WANT)] = .0093\n",
    "output[(VB, To)] = 0\n",
    "output[(VB, RACE)] = .00012\n",
    "\n",
    "output[(TO, I)] = 0\n",
    "output[(TO, WANT)] = 0\n",
    "output[(TO, To)] = .99\n",
    "output[(TO, RACE)] = 0\n",
    "\n",
    "output[(NN, I)] = 0\n",
    "output[(NN, WANT)] = .000054\n",
    "output[(NN, To)] = 0\n",
    "output[(NN, RACE)] = .00057\n",
    "\n",
    "output[(PPSS, I)] = .37\n",
    "output[(PPSS, WANT)] = 0\n",
    "output[(PPSS, To)] = 0\n",
    "output[(PPSS, RACE)] = 0\n",
    "\n",
    "\n",
    "sentence = [I, WANT, To, RACE]\n",
    "words = [\"I\", \"WANT\", \"TO\", \"RACE\"]\n",
    "\n",
    "# manage cell values and back pointers\n",
    "cells = {}\n",
    "backStates = {}\n",
    "\n",
    "def computeMaxPrev(t, sNext):\n",
    "    maxValue = 0\n",
    "    maxState = 0\n",
    "    \n",
    "    for s in range(stateCount):\n",
    "        value = cells[t, s] * trans[(s, sNext)]\n",
    "        if (s == 0 or value > maxValue):\n",
    "            maxValue = value\n",
    "            maxState = s\n",
    "            \n",
    "    return (maxValue, maxState)\n",
    "    \n",
    "def viterbi(trans, output, sentence):\n",
    "\n",
    "    # special handling for t=0 which have no prior states)\n",
    "    for s in range(stateCount):\n",
    "        cells[(0, s)] = trans[(start, s)] * output[(s, sentence[0])]\n",
    "        \n",
    "    # handle rest of time steps\n",
    "    for t in range(1, timeSteps):\n",
    "        for s in range(stateCount):\n",
    "            maxValue, maxState = computeMaxPrev(t-1, s)\n",
    "            backStates[(t,s)] = maxState\n",
    "            cells[(t, s)] = maxValue * output[(s, sentence[t])]\n",
    "            #print(\"t=\", t, \"s=\", s, \"maxValue=\", maxValue, \"maxState=\", maxState, \"output=\", output[(s, sentence[t])], \"equals=\", cells[(t, s)])\n",
    "        \n",
    "    # walk thru cells backwards to get most probable path\n",
    "    path = []\n",
    "    \n",
    "    for tt in range(timeSteps):\n",
    "        t = timeSteps - tt - 1    # step t backwards over timesteps\n",
    "        maxValue = 0\n",
    "        maxState = 0\n",
    "        \n",
    "        for s in range(stateCount):\n",
    "            value = cells[t, s] \n",
    "            if (s == 0 or value > maxValue):\n",
    "                maxValue = value\n",
    "                maxState = s\n",
    "                \n",
    "        path.insert(0, maxState)\n",
    "        \n",
    "    return path\n",
    "\n",
    "# test our algorithm on the POS TAG data\n",
    "path = viterbi(trans, output, sentence)\n",
    "\n",
    "print(\"Tagged Sentence:\")\n",
    "for i in range(timeSteps):\n",
    "    state = path[i]\n",
    "    print(\"  word=\", words[i], \"\\ttag=\", stateNames[state])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical NLP Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainData accuracy= 0.9211644374508261\n",
      "testData accuracy= 0.842039018250472\n",
      "name= raj classifed as= male\n",
      "name= nancy classifed as= female\n",
      "name= alex classifed as= female\n",
      "Most Informative Features\n",
      "                 suffix2 = 'na'           female : male   =     89.2 : 1.0\n",
      "                 suffix2 = 'ta'           female : male   =     39.2 : 1.0\n",
      "                 suffix2 = 'ld'             male : female =     36.8 : 1.0\n",
      "                  suffix = 'k'              male : female =     36.7 : 1.0\n",
      "                 suffix2 = 'ia'           female : male   =     33.9 : 1.0\n",
      "                  suffix = 'a'            female : male   =     33.5 : 1.0\n",
      "                 suffix2 = 'sa'           female : male   =     31.3 : 1.0\n",
      "                 suffix2 = 'rt'             male : female =     30.0 : 1.0\n",
      "                 suffix2 = 'rd'             male : female =     29.0 : 1.0\n",
      "                 suffix2 = 'do'             male : female =     25.8 : 1.0\n",
      "                 suffix3 = 'ard'            male : female =     25.5 : 1.0\n",
      "                  suffix = 'f'              male : female =     24.1 : 1.0\n",
      "                 suffix3 = 'ana'          female : male   =     23.6 : 1.0\n",
      "                 suffix2 = 'us'             male : female =     23.3 : 1.0\n",
      "                 suffix2 = 'ra'           female : male   =     22.6 : 1.0\n",
      "                 suffix3 = 'ita'          female : male   =     21.5 : 1.0\n",
      "                 suffix3 = 'nne'          female : male   =     18.1 : 1.0\n",
      "                 suffix3 = 'old'            male : female =     16.9 : 1.0\n",
      "                 suffix3 = 'vin'            male : female =     15.9 : 1.0\n",
      "                 prefix4 = 'mari'         female : male   =     15.5 : 1.0\n",
      "                 prefix3 = 'ros'          female : male   =     15.3 : 1.0\n",
      "                 suffix3 = 'ert'            male : female =     15.1 : 1.0\n",
      "                 prefix2 = 'hu'             male : female =     14.9 : 1.0\n",
      "                 suffix3 = 'dra'          female : male   =     13.7 : 1.0\n",
      "                 suffix2 = 'ka'           female : male   =     13.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# code to build a classifier to classify names as male or female\n",
    "# demonstrates the basics of feature extraction and model building\n",
    "\n",
    "names = [(name, 'male') for name in nltk.corpus.names.words(\"male.txt\")]\n",
    "names += [(name, 'female') for name in nltk.corpus.names.words(\"female.txt\")]\n",
    "\n",
    "def extract_gender_features(name):\n",
    "    name = name.lower()\n",
    "    features = {}\n",
    "    features[\"suffix\"] = name[-1:]\n",
    "    features[\"suffix2\"] = name[-2:] if len(name) > 1 else name[0]\n",
    "    features[\"suffix3\"] = name[-3:] if len(name) > 2 else name[0]\n",
    "    #features[\"suffix4\"] = name[-4:] if len(name) > 3 else name[0]\n",
    "    #features[\"suffix5\"] = name[-5:] if len(name) > 4 else name[0]\n",
    "    #features[\"suffix6\"] = name[-6:] if len(name) > 5 else name[0]\n",
    "    features[\"prefix\"] = name[:1]\n",
    "    features[\"prefix2\"] = name[:2] if len(name) > 1 else name[0]\n",
    "    features[\"prefix3\"] = name[:3] if len(name) > 2 else name[0]\n",
    "    features[\"prefix4\"] = name[:4] if len(name) > 3 else name[0]\n",
    "    features[\"prefix5\"] = name[:5] if len(name) > 4 else name[0]\n",
    "    #features[\"wordLen\"] = len(name)\n",
    "    \n",
    "    #for letter in \"abcdefghijklmnopqrstuvwyxz\":\n",
    "    #    features[letter + \"-count\"] = name.count(letter)\n",
    "   \n",
    "    return features\n",
    "\n",
    "data = [(extract_gender_features(name), gender) for (name,gender) in names]\n",
    "\n",
    "import random\n",
    "random.shuffle(data)\n",
    "\n",
    "#print(data[:10])\n",
    "#print()\n",
    "#print(data[-10:])\n",
    "\n",
    "dataCount = len(data)\n",
    "trainCount = int(.8*dataCount)\n",
    "\n",
    "trainData = data[:trainCount]\n",
    "testData = data[trainCount:]\n",
    "bayes = nltk.NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "def classify(name):\n",
    "    label = bayes.classify(extract_gender_features(name))\n",
    "    print(\"name=\", name, \"classifed as=\", label)\n",
    "\n",
    "print(\"trainData accuracy=\", nltk.classify.accuracy(bayes, trainData))\n",
    "print(\"testData accuracy=\", nltk.classify.accuracy(bayes, testData))\n",
    "\n",
    "classify('raj')\n",
    "classify('nancy')\n",
    "classify('alex')\n",
    "\n",
    "bayes.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie reviews / sentiment analysis - part #1\n",
    "from nltk.corpus import movie_reviews as reviews\n",
    "import random\n",
    "\n",
    "docs = [(list(reviews.words(id)), cat)  for cat in reviews.categories() for id in reviews.fileids(cat)]\n",
    "random.shuffle(docs)\n",
    "\n",
    "#print([ (len(d[0]), d[0][:2], d[1]) for d in docs[:10]])\n",
    "\n",
    "fd = nltk.FreqDist(word.lower() for word in reviews.words())\n",
    "topKeys = [ key for (key,value) in fd.most_common(2000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy= 0.868125\n",
      "test accuracy= 0.81\n",
      "Most Informative Features\n",
      "                   mulan = True              pos : neg    =      8.3 : 1.0\n",
      "             outstanding = True              pos : neg    =      8.1 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.5 : 1.0\n",
      "                  wasted = True              neg : pos    =      6.8 : 1.0\n",
      "                   awful = True              neg : pos    =      6.3 : 1.0\n",
      "                   damon = True              pos : neg    =      6.1 : 1.0\n",
      "                  allows = True              pos : neg    =      5.1 : 1.0\n",
      "                   bland = True              neg : pos    =      5.0 : 1.0\n",
      "                   worst = True              neg : pos    =      4.8 : 1.0\n",
      "                  poorly = True              neg : pos    =      4.7 : 1.0\n",
      "                 sandler = True              neg : pos    =      4.6 : 1.0\n",
      "                   waste = True              neg : pos    =      4.6 : 1.0\n",
      "               portrayed = True              pos : neg    =      4.4 : 1.0\n",
      "              ridiculous = True              neg : pos    =      4.3 : 1.0\n",
      "               pointless = True              neg : pos    =      4.3 : 1.0\n",
      "                    lame = True              neg : pos    =      4.3 : 1.0\n",
      "                terrific = True              pos : neg    =      4.2 : 1.0\n",
      "               laughable = True              neg : pos    =      4.1 : 1.0\n",
      "                     era = True              pos : neg    =      4.0 : 1.0\n",
      "                  stupid = True              neg : pos    =      4.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# movie reviews sentiment analysis - part #2\n",
    "import nltk\n",
    "\n",
    "\n",
    "def review_features(doc):\n",
    "    docSet = set(doc)\n",
    "    features = {}\n",
    "    \n",
    "    for word in topKeys:\n",
    "        features[word] = (word in docSet)\n",
    "        \n",
    "    return features\n",
    "\n",
    "#review_features(reviews.words(\"pos/cv957_8737.txt\"))\n",
    "\n",
    "data = [(review_features(doc), label) for (doc,label) in docs]\n",
    "\n",
    "dataCount = len(data)\n",
    "trainCount = int(.8*dataCount)\n",
    "\n",
    "trainData = data[:trainCount]\n",
    "testData = data[trainCount:]\n",
    "bayes2 = nltk.NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "print(\"train accuracy=\", nltk.classify.accuracy(bayes2, trainData))\n",
    "print(\"test accuracy=\", nltk.classify.accuracy(bayes2, testData))\n",
    "\n",
    "bayes2.show_most_informative_features(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
