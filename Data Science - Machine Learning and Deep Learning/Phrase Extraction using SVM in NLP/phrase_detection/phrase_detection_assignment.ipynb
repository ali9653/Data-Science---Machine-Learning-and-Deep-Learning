{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "data = pd.read_csv(\"training_data.tsv\", delimiter = '\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Remind me to carry a bottle when i go out at 2.00 pm'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out any random sentence from the training data-set\n",
    "sentence = data['sent'][60]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textblob:\n",
    "**Textblob is a library which has ready-made function to extract noun-phrases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTBLOB\n",
      "remind\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "print (\"TEXTBLOB\")\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "for np in blob.noun_phrases:\n",
    "    print (np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see that the results are very unsatisfactory, so we cannot use this approach. We will now use Approach 2 in which we will build a Support Vector Machine Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Training and testing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries and packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "df=pd.read_csv(\"training_data.tsv\",delimiter='\\t')\n",
    "df=df.dropna()\n",
    "y=df['label']\n",
    "X=df['sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing Data of X\n",
    "x=list()\n",
    "for _ in X:\n",
    "    t=re.findall('[a-z]+',_.lower())\n",
    "    t=[word for word in t if not word in set(stopwords.words('english'))]\n",
    "    x.append(\" \".join(t))\n",
    "x=pd.Series(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing data of y\n",
    "Y=list()\n",
    "for _ in y:\n",
    "    if _ ==\"Not Found\":\n",
    "        Y.append(\"Not Found\")\n",
    "    else:\n",
    "        Y.append(\"Found\")\n",
    "y=pd.Series(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data for train(75%) and test(25%)\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making list of x\n",
    "x_train_list=list()\n",
    "for i in X_train:\n",
    "    x_train_list.append(i)\n",
    "x_test_list=list()\n",
    "for j in X_test:\n",
    "    x_test_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<2455x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9256 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizing x\n",
    "vectorizer = CountVectorizer(max_features=1000) # n-grams Bag of word\n",
    "train_data = vectorizer.fit_transform(x_train_list) # expects a list of strings\n",
    "np.asarray(train_data)\n",
    "test_data = vectorizer.fit_transform(x_test_list) # expects a list of strings\n",
    "np.asarray(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Building (Support Vector Machine)\n",
    "classifier=svm.LinearSVC()\n",
    "classifier=classifier.fit(train_data,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Model in pickle\n",
    "f=open(\"svm_model.pickle\",'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting results\n",
    "result=classifier.predict(test_data)\n",
    "result_list=list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1183  295]\n",
      " [ 603  374]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Found       0.66      0.80      0.72      1478\n",
      "   Not Found       0.56      0.38      0.45       977\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      2455\n",
      "   macro avg       0.61      0.59      0.59      2455\n",
      "weighted avg       0.62      0.63      0.62      2455\n",
      "\n",
      "Accuracy of the model:  0.6342158859470468\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "cfm=confusion_matrix(y_test,result)\n",
    "print(cfm)\n",
    "\n",
    "print(\"Classification report: \")\n",
    "\n",
    "\n",
    "print(classification_report(y_test,result))\n",
    "\n",
    "acc=accuracy_score(y_test,result)\n",
    "print(\"Accuracy of the model: \",acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Testing the 'svm_model' on 'eval_data.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading trained model\n",
    "f=open(\"svm_model.pickle\",'rb')\n",
    "clf=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading eval file\n",
    "eval_data=open(\"eval_data.txt\",'r')\n",
    "X=list()\n",
    "all_words=list()\n",
    "for lines in eval_data:\n",
    "    all_words+=(lines.strip('\\n').split(' '))\n",
    "    X.append(lines.strip('\\n'))\n",
    "eval_data.close()\n",
    "all_words=list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing and converting to array: X_test\n",
    "vectorizer=CountVectorizer(max_features=1000)\n",
    "X_test=vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting Result (found/not found)\n",
    "result=list(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_in_reminder=['i','me','at','to','date','time','for','tommorow','tonight','today',\\\n",
    "                           'sunday','monday','tuesday','wednesday','thursday','friday','saturday',\\\n",
    "                           'morning','evening']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For found, extract result text\n",
    "def extract(index):\n",
    "    #Create the extractor function\n",
    "    temp=X[index].lower()\n",
    "    t=re.findall('remi[a-z]+ me? to? (.+?) at',temp)\n",
    "    if len(t)==0:\n",
    "        t=re.findall('rem[a-z]+ me? to? (.+?) on',temp)\n",
    "    if len(t)==0:\n",
    "        t=temp\n",
    "        t+=' .y'\n",
    "        t=re.findall('remi[\\w.]+ (.+?) {}'.format(t.split(\" \")[len(t.split(\" \"))-1]),t)\n",
    "    if len(t)>0:\n",
    "        t=str(t[0]).split()    \n",
    "        t=[word for word in t if not word in common_words_in_reminder]\n",
    "    if len(t)>0:\n",
    "        result[index]=\" \".join(t)\n",
    "    else:\n",
    "        result[index]=\"Not Found\"\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if(result[i]==\"Found\"):\n",
    "        extract(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the final result\n",
    "file=open(\"final_result_eval.tsv\",'w')\n",
    "file.write(\"sent\\tlabel\\n\")\n",
    "for i in range(len(result)):\n",
    "    file.write(str(X[i])+'\\t'+str(result[i])+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
