#linear regression : project 1 : property price prediction
#whenever there is continuous variable use linear regression
#objective: predict property price which depends on economic and social factors
#sale price == target variable to be predicted based on given data

dfPP=read.csv(file.choose(),header = T)     #import train data

str(dfPP)
#no need of data cleaning as data is accurate

summary(dfPP)

#delete unwanted variables (years)

dfPP$Id=NULL
dfPP$Construction_Year=NULL
dfPP$Remodel_Year=NULL
dfPP$Garage_Built_Year=NULL
dfPP$Garage_Finish_Year=NULL

#find the missing values

mvalues=sapply(dfPP,function(x)sum(is.na(x)))    #sapply is applied to the whole data frame to find missing values all together for large data

sort(mvalues,decreasing = T)
mvalues

install.packages("missForest")      #this package will remove all missing values together at once
library(missForest)

#impute missing values

dfPP.imp=missForest(dfPP)

dfPP.imp$ximp

#create data frame of imputed values

dfPP.imp=dfPP.imp$ximp

#count NA's before and after

sapply(dfPP,function(x)sum(is.na(x)))     #before
sapply(dfPP.imp,function(x)sum(is.na(x)))       #after

#dfPP.imp is now cleaned and processed data file for linear regression model building


#study imputation of Pool-Quality before and after imputation

summary(dfPP$Pool_Quality)
summary(dfPP.imp$Pool_Quality)

#hot-coding of "category"
#only for categorical and factorial data

library(mltools)
install.packages("mltools")

library(data.table)

df5=one_hot(as.data.table(dfPP.imp))

#df5 will be the final file for linear regression model building
#Model Building

#Step:1
#split data into Train(trn) and Test(tst)

set.seed(123)
rnum=sample(nrow(df5),nrow(df5)*0.7)   #seperate 70% data for TRAIN model rest 30% for test
head(rnum)    #displays top 6 lines by default
#nrow(df5) counts no of rows in df5

rnum    #1021 random numbers generated by sample command

trn=df5[rnum,]    #1021 rows for training
tst=df5[-rnum,]    #rest data will go for testing

#bild Linear Model (LM) on Train(trn) data

l1=lm(trn$Sale_Price~.,data = trn)
summary(l1)

#residual median should be near to zero and if max and mean of residuals are equal then the model is a GOOD fit
#multiple r square should be equal to 1 (0.9335)  is near to 1 === good model
#if p-value is less than 0.05 === best fit model

#diagnostic plots for assumptions

dev.new()
par(mfrow=c(2,2))
plot(l1)

#prediction and accuracy (RMSE)

l1.pred=predict(l1,newdata = tst)
l1.pred




tst

RMSEl1=mean(sqrt((l1.pred-tst$Sale_Price)^2))
RMSEl1


#build 2nd model Random Forest Model == fetches most important data

install.packages("randomForest")
library(randomForest)

rf=randomForest(dfPP.imp$Sale_Price~.,data=dfPP.imp,importance=T)
rf
dev.new()
varImpPlot(rf)

imp=importance(rf)    #save importance list in imp
str(imp)

s1=data.frame(sort(imp[,1],decreasing = T))
s1

s2=row.names(s1)
s2     #extracted variables name

s3=s2[1:10]
s3     #selecting top 10 variables only

s4='Sale_Price'
rhs=paste(s3,collapse = ' + ')
rhs

f=as.formula(paste(s4,rhs,sep = " ~ "))
f

set.seed(123)
rnumrf=sample(nrow(dfPP.imp),nrow(dfPP.imp)*0.7)


trnrf=dfPP.imp[rnumrf,]
tstrf=dfPP.imp[-rnumrf,]

l2=lm(f,data = trnrf)
l2
l2.pred=predict(l2,newdata = tstrf)

RMSEl2=mean(sqrt((l2.pred-tstrf$Sale_Price)^2))
RMSEl2


#Conclusion : L1 is a better model as error rate is less















































































































































































































